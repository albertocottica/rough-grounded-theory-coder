[
  {
    "id": 86417,
    "username": "antonekker",
    "post_number": 1,
    "reply_to_post_number": null,
    "cooked": "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://edgeryders.eu/uploads/default/original/2X/4/458d24bf9be2f17cad7e1d4b95af3803c2e7accf.jpeg\" data-download-href=\"https://edgeryders.eu/uploads/default/458d24bf9be2f17cad7e1d4b95af3803c2e7accf\" title=\"surveillance_by_Pete_Linforth\"><img src=\"https://edgeryders.eu/uploads/default/optimized/2X/4/458d24bf9be2f17cad7e1d4b95af3803c2e7accf_2_517x310.jpeg\" alt=\"surveillance_by_Pete_Linforth\" data-base62-sha1=\"9VhlkNiULhXNB7fp4rcrCBEP3Px\" width=\"517\" height=\"310\" srcset=\"https://edgeryders.eu/uploads/default/optimized/2X/4/458d24bf9be2f17cad7e1d4b95af3803c2e7accf_2_517x310.jpeg, https://edgeryders.eu/uploads/default/optimized/2X/4/458d24bf9be2f17cad7e1d4b95af3803c2e7accf_2_775x465.jpeg 1.5x, https://edgeryders.eu/uploads/default/optimized/2X/4/458d24bf9be2f17cad7e1d4b95af3803c2e7accf_2_1034x620.jpeg 2x\" data-dominant-color=\"072C39\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">surveillance_by_Pete_Linforth</span><span class=\"informations\">1280×768 166 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>In 2014, the Dutch government introduced a legislation approving the use of a risk scoring algorithm to detect welfare fraud. This system, called System Risk Indication (SyRI), pools together data from various government agencies to calculate the likelihood of committing welfare or tax fraud. The UN Special Rapporteur on Extreme Poverty and Human Rights has described governments' use of similar systems as "<a href=\"https://www.ohchr.org/Documents/Issues/Poverty/A_74_48037_AdvanceUneditedVersion.docx\" rel=\"noopener nofollow ugc\">digital welfare states</a>" and condemned them for their lack of transparency and oversight, and discriminatory impacts. In the case of SyRI, we discovered that the system used neighbourhood data to profile against migrant and poor communities.</p>\n<p>With a coalition of privacy organisations, we challenged SyRI on grounds of privacy and equality violations. Earlier this year, the Dutch court found SyRI to be unlawful and ordered its immediate halt. You can read more <a href=\"https://algorithmwatch.org/en/story/syri-netherlands-algorithm/\" rel=\"noopener nofollow ugc\">here</a> and <a href=\"https://uitspraken.rechtspraak.nl/inziendocument?id=ECLI:NL:RBDHA:2020:1878\" rel=\"noopener nofollow ugc\">here</a>.</p>\n<p>This case sets a strong legal precedents for future cases. Unfortunately, the Dutch government recently introduced a new legislation with even more intrusive use of personal data for automated decision-making system—the fight continues!</p>\n<p>Join me and others this Wednesday at 6:30pm CET to AMA about automated decision-making systems and their impacts on our human rights.</p>\n<p><em>What is an AMA and how does it work?</em><br>\nAMA, or Ask Me Anything, is an <a href=\"https://mashable.com/2013/03/25/reddit-ama-tips/?europe=true\" rel=\"noopener nofollow ugc\">interviewing format</a> popularised via Reddit. In short, you ask me questions, and I answer them live for an hour.</p>\n<p><em>How does it work?</em><br>\nAnyone is welcome to post questions/comments below. On Wednesday, I'll come back to this thread to start answering questions I find interesting. I will do my best to reply to as many questions as possible, but please note that not all questions/comments will be addressed.</p>\n<p><em>Who can join?</em><br>\nAnyone! As the conversations get going during the hour, you will see multiple threads naturally emerging. There is an open invitation for everyone to contribute—please feel free to reach out to other community members, either on the thread or via DM, to continue the discussion!!</p>\n<p><em>photo credit: Pete Linforth</em></p>"
  },
  {
    "id": 86421,
    "username": "jeremyboom8",
    "post_number": 2,
    "reply_to_post_number": null,
    "cooked": "<p>This is awesome Anton! This is a great victory for all of us.</p>\n<p>I'd be curious to know if (and how) you are tying your goals to the Free Software/Open Source movement somehow? Have you maybe written about this somewhere?</p>\n<div class=\"youtube-onebox lazy-video-container\" data-video-id=\"iuVUzg6x2yo\" data-video-title=\"Public Money? Public Code!\" data-video-start-time=\"\" data-provider-name=\"youtube\">\n  <a href=\"https://www.youtube.com/watch?v=iuVUzg6x2yo\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    <img class=\"youtube-thumbnail\" src=\"https://img.youtube.com/vi/iuVUzg6x2yo/maxresdefault.jpg\" title=\"Public Money? Public Code!\" width=\"\" height=\"\">\n  </a>\n</div>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://publiccode.eu/\">\n  <header class=\"source\">\n      <img src=\"https://publiccode.eu/favicon.png\" class=\"site-icon\" width=\"\" height=\"\">\n\n      <a href=\"https://publiccode.eu/\" target=\"_blank\" rel=\"noopener nofollow ugc\">publiccode.eu</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://publiccode.eu/img/share-graphics/why.png\" class=\"thumbnail\" width=\"\" height=\"\">\n\n<h3><a href=\"https://publiccode.eu/\" target=\"_blank\" rel=\"noopener nofollow ugc\">Public Money, Public Code</a></h3>\n\n  <p>Public Money, Public Code - A campaign for releasing publicly financed software as Free Software</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"
  },
  {
    "id": 86428,
    "username": "hugi",
    "post_number": 3,
    "reply_to_post_number": null,
    "cooked": "<p>Hey <a class=\"mention\" href=\"/u/antonekker\">@antonekker</a>, thanks for doing this!</p>\n<p>At the NGI Forum last year, Nesta hosted <a href=\"https://edgeryders.eu/t/ngi-trustmark-notes-from-ngi-forum-workshop/10883/4\">a discussion on a potential "NGI Trustmark"</a> for AI. One of the issues brought up by an AI researcher was this:</p>\n<aside class=\"quote no-group\" data-username=\"hugi\" data-post=\"1\" data-topic=\"10883\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://edgeryders.eu/user_avatar/edgeryders.eu/hugi/48/16487_2.png\" class=\"avatar\"><a href=\"https://edgeryders.eu/t/ngi-trustmark-notes-from-ngi-forum-workshop/10883/1\">NGI Trustmark, notes from NGI Forum workshop</a></div>\n<blockquote>\n<ul>\n<li>AI bias is difficult because reducing bias often decreases accuracy. AI bias could be addressed by requiring companies to be upfront about what trade-offs they have accepted and how their AIs are biased. This is much like how human bias (internalized racism, sexism, etc) is addressed by becoming aware of one's own bias.</li>\n</ul>\n</blockquote>\n</aside>\n<p>How do you think this could work in practice? And is a Trustmark a good idea, or is it a waste of effort?</p>"
  },
  {
    "id": 86429,
    "username": "hugi",
    "post_number": 4,
    "reply_to_post_number": null,
    "cooked": "<p>Another idea I came up with during that discussion on Trusmarks was quite a radical proposal: That any AI trained on public data or data acquired in a public setting must be released to the public, by EU law. What do you think about that proposal?</p>"
  },
  {
    "id": 86441,
    "username": "nadia",
    "post_number": 5,
    "reply_to_post_number": 3,
    "cooked": "<p>ping <a class=\"mention\" href=\"/u/pbihr\">@pbihr</a> !</p>"
  },
  {
    "id": 86467,
    "username": "alberto",
    "post_number": 6,
    "reply_to_post_number": null,
    "cooked": "<p>Awesome, indeed. Welcome, <a class=\"mention\" href=\"/u/antonekker\">@antonekker</a>, and thank you so much for this. I'll be there… which means here. <img src=\"https://edgeryders.eu/images/emoji/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>"
  },
  {
    "id": 86468,
    "username": "alberto",
    "post_number": 7,
    "reply_to_post_number": 4,
    "cooked": "<aside class=\"quote no-group\" data-username=\"hugi\" data-post=\"4\" data-topic=\"14766\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://edgeryders.eu/user_avatar/edgeryders.eu/hugi/48/16487_2.png\" class=\"avatar\"> hugi:</div>\n<blockquote>\n<p>That any AI trained on public data or data acquired in a public setting must be released to the public, by EU law.</p>\n</blockquote>\n</aside>\n<p>Promising, but… what does it mean, exactly, to "release an AI"?</p>\n<aside class=\"quote no-group\" data-username=\"antonekker\" data-post=\"1\" data-topic=\"14766\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://edgeryders.eu/letter_avatar_proxy/v4/letter/a/6a8cbe/48.png\" class=\"avatar\"> antonekker:</div>\n<blockquote>\n<p>In 2014, the Dutch government introduced a legislation approving the use of a risk scoring algorithm to detect welfare fraud. This system, called System Risk Indication (SyRI), pools together data from various government agencies to calculate the likelihood of committing welfare or tax fraud. The UN Special Rapporteur on Extreme Poverty and Human Rights has described governments' use of similar systems as "<a href=\"https://www.ohchr.org/Documents/Issues/Poverty/A_74_48037_AdvanceUneditedVersion.docx\">digital welfare states </a>" and condemned them for their lack of transparency and oversight, and discriminatory impacts. In the case of SyRI, we discovered that the system used neighbourhood data to profile against migrant and poor communities.</p>\n</blockquote>\n</aside>\n<p>I wonder if <a class=\"mention\" href=\"/u/yudhanjaya\">@yudhanjaya</a> knew about this… right up your street!</p>"
  },
  {
    "id": 86472,
    "username": "hugi",
    "post_number": 8,
    "reply_to_post_number": 7,
    "cooked": "<aside class=\"quote no-group\" data-username=\"alberto\" data-post=\"7\" data-topic=\"14766\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://edgeryders.eu/user_avatar/edgeryders.eu/alberto/48/5_2.png\" class=\"avatar\"> alberto:</div>\n<blockquote>\n<p>Promising, but… what does it mean, exactly, to "release an AI"?</p>\n</blockquote>\n</aside>\n<p>To release the input variables and the code that generates the AI. It is, after all, deterministic. This does not have to be the same thing as releasing private data if the data is anonymized and scrambled. By doing so, anyone with the computing power required could test the bias of the AI by feeding it test data.</p>"
  },
  {
    "id": 86474,
    "username": "alberto",
    "post_number": 9,
    "reply_to_post_number": null,
    "cooked": "<p><a class=\"mention\" href=\"/u/antonekker\">@antonekker</a>: could you explain the mechanism whereby, exactly, SyRI targeted especially the poor? After all, if you are looking at social welfare fraud, most people using social welfare are likely to be poor.</p>\n<p>And another question: what was, in your opinion, what made the court decide to order SyRI's shutdown? Was it opacity? Or lack of fairness? Or what?</p>"
  },
  {
    "id": 86480,
    "username": "johncoate",
    "post_number": 11,
    "reply_to_post_number": null,
    "cooked": "<p>It appears that the mere act of using algorithms per se is not the main issue, unless they discriminate, profile unfairly, do not disclose, get permission or otherwise violate the European Convention on Human Rights and/or the GDPR. Algorithms are used to determine someone's credit worthiness and algorithms get used in crime fighting, and other purposes.</p>\n<p>Do you think the government will try to make some adjustments to comply with article 8 and the GDPR and then try to carry on with the program? It seems on reading the ruling that the Court is open to it. ("The court shares the position of the State that those new technological possibilities to prevent and combat fraud should be used.")</p>"
  },
  {
    "id": 86484,
    "username": "johncoate",
    "post_number": 12,
    "reply_to_post_number": null,
    "cooked": "<p>I found this part of the Judgement to be fascinating. It seems to me to be saying that profiling is pretty accurate, but we aren't sure why since it isn't based on substantive merits.  Thus there are no admissible reasons that justify it.  So, it's bad to profile people, but it works pretty well.  But just because you can doesn't mean you should. This is a fundamental dynamic.  It seems like you almost should need a warrant to do it, like wiretapping.</p>\n<blockquote>\n<p>\"The term "self-learning" is confusing and misleading: an algorithm does not know and understand reality. There are predictive algorithms which are fairly accurate in predicting the outcome of a court case. However, they do not do so on the basis of the substantive merits of the case. They can therefore not substantiate their predictions in a legally sound manner, while that is required for all legal proceedings for each individual case. (…)</p>\n</blockquote>\n<blockquote>\n<p>The reverse also applies: the human user of such a self-learning system does not understand why the system concludes that there is a link. An administrative organ that partially bases its actions on such a system is unable to properly justify its actions and to properly substantiate its decisions."</p>\n</blockquote>"
  },
  {
    "id": 86487,
    "username": "johncoate",
    "post_number": 13,
    "reply_to_post_number": null,
    "cooked": "<p>Your current case is on behalf of Uber drivers who were fired by algorithm decision making that also significantly identifies the reason as "fraud" which triggers deeper problems for the drivers.</p>\n<p>Uber is famously bad to its drivers, but what is the outcome you hope to achieve with this case?  Are you looking to prevent all machine-generated judgements against individuals?  Or is your focus on ensuring fair due process for people who are the recipients of such judgements?</p>"
  },
  {
    "id": 86540,
    "username": "katejsim",
    "post_number": 14,
    "reply_to_post_number": null,
    "cooked": "<p>What were the biggest challenges in bringing your case to the court? Given how "blackboxed" automated systems can be, was it difficult to explain and convince the court about how they work and how they influence people?</p>"
  },
  {
    "id": 86581,
    "username": "Leonie",
    "post_number": 15,
    "reply_to_post_number": null,
    "cooked": "<p>Hi <a class=\"mention\" href=\"/u/antonekker\">@antonekker</a>! Thank you for sharing your work with us!<br>\nAccording to Algorithm Watch, SyRI wants to find "unlikely citizen profiles" - can you tell us more about how those profiles are generated/ how "likely citizen profiles" are created? Is this based on historical data about previous welfare/tax fraud? And, does SyRI include sociodemographic data that is unrelated to a history of welfare/tax fraud?</p>"
  },
  {
    "id": 86582,
    "username": "Leonie",
    "post_number": 16,
    "reply_to_post_number": null,
    "cooked": "<aside class=\"quote no-group\" data-username=\"antonekker\" data-post=\"1\" data-topic=\"14766\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://edgeryders.eu/letter_avatar_proxy/v4/letter/a/6a8cbe/48.png\" class=\"avatar\"> antonekker:</div>\n<blockquote>\n<p>Unfortunately, the Dutch government recently introduced a new legislation with even more intrusive use of personal data for automated decision-making system—the fight continues!</p>\n</blockquote>\n</aside>\n<p>SyRI already sounded scary! What concerns you the most about this new tool?</p>"
  },
  {
    "id": 86645,
    "username": "antonekker",
    "post_number": 17,
    "reply_to_post_number": 9,
    "cooked": "<p>Hi Alberto! Thanks for your interesting question.</p>\n<p>The SyRI system was used in 'SyRI-projects'. These projects were targeted at specific neighborhoods that were considered 'problem districts'. Therefore, the profiling that took place in SyRI mostly affected groups with a lower socio-economic status and or minority / immigration background.</p>\n<p>The court also mentions the 'Echo-chamber' effect: irregularities in the targeted areas might reinforce a negative image of its occupants. This might lead to stereotyping.</p>\n<p>The Court's decision to end the use of SyRI was primarily based on the European Convention on Human Rights (ECHR). The use of SyRI was insufficiently transparent and verifiable. Also, it violated a number of privacy principles, such as 'purpose limitation' and data minimization. Therefore, the use of SyRI cannot always be considered proportionate and necessary.</p>"
  },
  {
    "id": 86647,
    "username": "antonekker",
    "post_number": 18,
    "reply_to_post_number": 11,
    "cooked": "<p>John:</p>\n<p>Hi John, actually I'm quite sure that the government will try to make adjustments to carry on with similar projects. After the judgment was given, the Dutch State decided not to appeal it. At first, my clients were very surprised. However, shortly after that, the government introduced a new legislative proposal that provides a general framework for SyRI like systems. We call it 'Super SyRI'.</p>\n<p>Under the new law, risk profiling technologies can be introduced in several domains. The law only provides a general framework. Specific requirements will be set in by ministerial decree, which by itself is problematic from a constitutional perspective.</p>"
  },
  {
    "id": 86648,
    "username": "katejsim",
    "post_number": 19,
    "reply_to_post_number": null,
    "cooked": "<p>On behalf of <a class=\"mention\" href=\"/u/ccs\">@CCS</a>:</p>\n<p>Where to systems like SyRI originate? Is there a pattern to that? Did it originate w the people subjected to it (government agency) or with others (like digital rights NGOs)? How do we create the necessary ties so that cases not on the radar of tech orgs + wired politicians get addressed as well?</p>"
  },
  {
    "id": 86649,
    "username": "alberto",
    "post_number": 20,
    "reply_to_post_number": 17,
    "cooked": "<aside class=\"quote no-group\" data-username=\"antonekker\" data-post=\"17\" data-topic=\"14766\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://edgeryders.eu/letter_avatar_proxy/v4/letter/a/6a8cbe/48.png\" class=\"avatar\"> antonekker:</div>\n<blockquote>\n<p>These projects were targeted at specific neighborhoods that were considered 'problem districts'.</p>\n</blockquote>\n</aside>\n<p>Here I have two further questions (sorry, Anton!).</p>\n<ol>\n<li>On the basis of what information were they considered "problem districts"?</li>\n<li>I can imagine the following mechanism for unfairly affecting the groups you mention: if you look for irregularities, you tend to find them (with some probability). According to some observers, the American police searches mostly black young men, which results in black young men accounting for a disproportionately high number of positive searches (and also negative ones). Was this what worried the courts? Solution: unleash SyRI on randomly chosen citizens.</li>\n</ol>\n<aside class=\"quote no-group\" data-username=\"antonekker\" data-post=\"17\" data-topic=\"14766\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://edgeryders.eu/letter_avatar_proxy/v4/letter/a/6a8cbe/48.png\" class=\"avatar\"> antonekker:</div>\n<blockquote>\n<p>The use of SyRI was insufficiently transparent and verifiable. Also, it violated a number of privacy principles, such as 'purpose limitation' and data minimization.</p>\n</blockquote>\n</aside>\n<p>Clear, thanks.</p>"
  },
  {
    "id": 86650,
    "username": "antonekker",
    "post_number": 21,
    "reply_to_post_number": 2,
    "cooked": "<p>Hey there Jeremy!</p>\n<p>Currently, there is no connection to the Free Software / Open Source movement. However, certain aspects of algorithmic decision might be addressed in a way that resembles the 'open source approach'. I'm thinking about standards for how to prevent bias and discrimination, how to assess the impact of algorithms and how to explain the outcomes. Such standards might be assessed and improved within the public domain. There are many different societal contexts and use cases that would have to be addressed, for instance financial sector, automotive, health care, et.</p>\n<p>Does that seem plausible to you?</p>"
  }
]